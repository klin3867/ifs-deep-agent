# DeepAgent - Copilot Instructions

## Project Overview

DeepAgent is a **deep reasoning agent framework** that enables LLMs to autonomously think, discover tools, and execute actions in a single coherent reasoning process. It supports tool-use benchmarks (ToolBench, API-Bank, RestBench, ToolHop), embodied AI (ALFWorld), web navigation (WebShop), and deep research tasks (GAIA, HLE).

## Architecture

### Core Components

- **[src/run_deep_agent.py](src/run_deep_agent.py)** - Main entry point and reasoning loop orchestrator
- **[src/tools/tool_manager.py](src/tools/tool_manager.py)** - Central hub for tool retrieval and execution across all datasets
- **[src/prompts/prompts_deepagent.py](src/prompts/prompts_deepagent.py)** - Prompt templates with special markers for tool calls
- **[src/evaluate/](src/evaluate/)** - Dataset-specific evaluation scripts

### Data Flow

1. User query → Prompt template (`prompts_deepagent.py`) → Reasoning LLM (vLLM-served)
2. LLM outputs tool markers → `ToolManager` routes to dataset-specific caller
3. Tool response → Injected back into reasoning context → Continue until `\boxed{ANSWER}`

### Key Special Markers (defined in `prompts_deepagent.py`)

```python
BEGIN_TOOL_SEARCH = "<tool_search>"     # Search for tools
END_TOOL_SEARCH = "</tool_search>"
BEGIN_TOOL_CALL = "<tool_call>"          # Execute tool with JSON args
END_TOOL_CALL = "</tool_call>"
BEGIN_TOOL_RESPONSE = "<tool_call_result>"
END_TOOL_RESPONSE = "</tool_call_result>"
FOLD_THOUGHT = "<fold_thought>"          # Trigger memory consolidation
```

## Configuration

All settings live in **[config/base_config.yaml](config/base_config.yaml)**:
- **API keys**: `toolbench_api`, `google_serper_api`, `jina_api_key`, `tmdb_access_token`, `spotipy_*`
- **Model endpoints**: `base_url`, `aux_base_url`, `vqa_base_url` (supports OpenAI API or vLLM `/v1`)
- **Tokenizer paths**: Required for vLLM models; auto-detected for OpenAI
- **Data paths**: `{dataset}_data_path` patterns for benchmark data

### OpenAI API Support

DeepAgent auto-detects OpenAI models (based on `base_url` containing `openai.com` or model names like `gpt-*`):
- Uses `chat.completions` API instead of vLLM's `completions` API
- Skips HuggingFace tokenizer loading
- Set `tokenizer_path` to the model name (e.g., `gpt-5-mini`) for OpenAI models

## Running the Agent

```bash
# Tool search enabled (open-set)
python src/run_deep_agent.py --config_path ./config/base_config.yaml \
    --dataset_name toolbench --enable_tool_search --eval

# Closed-set mode (predefined tools)
python src/run_deep_agent.py --config_path ./config/base_config.yaml \
    --dataset_name gaia --eval
```

Key flags: `--enable_tool_search`, `--enable_thought_folding`, `--max_action_limit`, `--concurrent_limit`

## Adding New Datasets

1. Add data path in `config/base_config.yaml` as `{dataset}_data_path`
2. In `tool_manager.py._initialize()`: Add dataset-specific caller initialization
3. In `run_deep_agent.py` prepare section (~line 760): Add question extraction and tool list construction
4. Create evaluation in `src/evaluate/evaluate_{dataset}.py` following `evaluate_base.py` patterns

## Tool Implementation Patterns

Each dataset has its own tool caller in `src/tools/`:

| Dataset | Caller | Notes |
|---------|--------|-------|
| ToolBench | `rapid_api.RapidAPICaller` | 16k+ RapidAPIs via external service |
| API-Bank | `api_bank.APIBankExecutor` | Local Python-based API simulation |
| RestBench | `restbench_api` | TMDB/Spotify REST APIs |
| ToolHop | `toolhop.ToolHopCaller` | Dynamic function execution |
| ALFWorld | `envs/alfworld.ALFWorldEnvWrapper` | Text-based embodied env |
| WebShop | `envs/webshop.WebshopEnvWrapper` | Web shopping simulation |

Tool definitions use OpenAI function-calling format (`openai_function` dict with `name`, `description`, `parameters`).

## Memory Folding System

When reasoning becomes lengthy, the agent can trigger `<fold_thought>` to consolidate:
- **Episodic Memory**: High-level log of key events
- **Working Memory**: Current sub-goal and near-term plans  
- **Tool Memory**: Learned tool usage patterns

Generated by auxiliary model via `run_thought_folding()` in `run_deep_agent.py`.

## Testing & Evaluation

- Run with `--eval` flag to auto-evaluate after generation
- Evaluation scripts in `src/evaluate/` use `extract_answer_fn()` to parse `\boxed{...}` answers
- Outputs saved to `./outputs/{dataset}.{model}.deepagent/`

## Dependencies

- **vLLM**: Required for serving reasoning models (QwQ, Qwen3)
- **Transformers**: Tokenizer for chat template formatting
- **sentence-transformers**: Tool retriever embeddings
- **alfworld**: ALFWorld environment (install separately if needed)
- **httpx + httpx-sse**: For MCP client connections

Start tool retriever server for open-set tool search:
```bash
python src/run_tool_search_server.py --base_config_path ./config/base_config.yaml \
    --datasets toolbench,toolhop,tmdb,spotify,api_bank --port 8001
```

## MCP Integration (IFS Cloud)

DeepAgent can connect to MCP (Model Context Protocol) servers for IFS Cloud APIs. MCP is JSON-RPC 2.0 over HTTP/SSE.

### Available MCP Servers

| Server | Endpoint | Purpose |
|--------|----------|---------|
| Planning | `http://localhost:9001/sse` | Inventory, supply/demand, MRP, FEFO |
| Customer | `http://localhost:9002/sse` | Orders, customers, reservations |

### MCP Handshake (Required)

MCP requires a 2-step handshake before tool calls:

```python
# 1. Connect to SSE, receive endpoint event
# 2. Send initialize request
{"jsonrpc": "2.0", "id": 1, "method": "initialize", 
 "params": {"protocolVersion": "2024-11-05", "capabilities": {}, 
            "clientInfo": {"name": "deepagent", "version": "1.0"}}}
# 3. Send initialized notification (no id)
{"jsonrpc": "2.0", "method": "notifications/initialized"}
# 4. Now you can call tools/list and tools/call
```

### Compact Tool Registry (Token Optimization)

Full MCP tool schemas consume ~30,000 tokens for 60 tools. DeepAgent uses a compact registry approach:

**[src/tools/mcp_tool_registry.py](src/tools/mcp_tool_registry.py)** - Static registry with one-line summaries (~1,200 tokens total)

```python
from tools.mcp_tool_registry import build_tool_prompt, get_tool_for_execution

# Get compact tool list for LLM prompt (~1,200 tokens)
prompt_tools = build_tool_prompt()

# Get tool metadata for execution routing
tool = get_tool_for_execution("health_check")  # Returns ToolSummary
```

Categories: `system`, `customers`, `parts`, `orders`, `inventory`, `reservations`, `shipments`, `planning`, `exceptions`

### Running MCP with DeepAgent

```bash
# Ensure MCP servers are running on ports 9001/9002
python src/run_deep_agent.py --config_path ./config/base_config.yaml \
    --single_question "Show me any past due customer order lines" \
    --dataset_name mcp --max_action_limit 5
```

### Running MCP Servers Locally

```bash
# Source IFS credentials
source config/ifs.env

# Start with supergateway (stdio → SSE bridge)
npx -y supergateway --port 9001 --stdio "docker run --rm -i \
  -e IFS_HOST=$IFS_HOST -e IFS_TENANT=$IFS_TENANT \
  -e IFS_CLIENT_ID=$IFS_CLIENT_ID -e IFS_CLIENT_SECRET=$IFS_CLIENT_SECRET \
  ifs-planning-manager-mcp:latest"
```

Environment variables are defined in [config/ifs.env](config/ifs.env).

### Common MCP Gotchas

- **"Not initialized" error**: Must complete handshake first
- **Session ID changes**: Each SSE connection gets new session; don't reuse endpoints
- **Timeouts**: IFS operations can be slow; use 60-120s timeout
- **Token limits**: Use compact tool registry, not full JSON schemas
